Automatically generated by Mendeley Desktop 1.17.11
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Angelopoulos2021,
	title={A gentle introduction to conformal prediction and distribution-free uncertainty quantification},
	author={Angelopoulos, Anastasios N and Bates, Stephen},
	journal={arXiv preprint arXiv:2107.07511},
	year={2021}
}

@article{Mejia2020,
	title={A Bayesian general linear modeling approach to cortical surface fMRI data analysis},
	author={Mejia, Amanda F and Yue, Yu and Bolin, David and Lindgren, Finn and Lindquist, Martin A},
	journal={Journal of the American Statistical Association},
	volume={115},
	number={530},
	pages={501--520},
	year={2020},
	publisher={Taylor \& Francis}
}

@article{chen2017density,
	title={Density level sets: Asymptotics, inference, and visualization},
	author={Chen, Yen-Chi and Genovese, Christopher R and Wasserman, Larry},
	journal={Journal of the American Statistical Association},
	volume={112},
	number={520},
	pages={1684--1696},
	year={2017},
	publisher={Taylor \& Francis}
}

@article{Wieslander2020,
	title={Deep learning with conformal prediction for hierarchical analysis of large-scale whole-slide tissue images},
	author={Wieslander, H{\aa}kan and Harrison, Philip J and Skogberg, Gabriel and Jackson, Sonya and Frid{\'e}n, Markus and Karlsson, Johan and Spjuth, Ola and W{\"a}hlby, Carolina},
	journal={IEEE journal of biomedical and health informatics},
	volume={25},
	number={2},
	pages={371--380},
	year={2020},
	publisher={IEEE}
}

@article{Borgefors1986,
	title={Distance transformations in digital images},
	author={Borgefors, Gunilla},
	journal={Computer vision, graphics, and image processing},
	volume={34},
	number={3},
	pages={344--371},
	year={1986},
	publisher={Elsevier}
}

@inproceedings{Brunekreef2024,
	title={Kandinsky Conformal Prediction: Efficient Calibration of Image Segmentation Algorithms},
	author={Brunekreef, Joren and Marcus, Eric and Sheombarsing, Ray and Sonke, Jan-Jakob and Teuwen, Jonas},
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages={4135--4143},
	year={2024}
}

@article{Marandon2024,
	title={Conformal link prediction for false discovery rate control},
	author={Marandon, Ariane},
	journal={TEST},
	pages={1--22},
	year={2024},
	publisher={Springer}
}

@inproceedings{Guo2017,
	title={On calibration of modern neural networks},
	author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
	booktitle={International conference on machine learning},
	pages={1321--1330},
	year={2017},
	organization={PMLR}
}

@article{Gupta2020,
	title={Distribution-free binary classification: prediction sets, confidence intervals and calibration},
	author={Gupta, Chirag and Podkopaev, Aleksandr and Ramdas, Aaditya},
	journal={Advances in Neural Information Processing Systems},
	volume={33},
	pages={3711--3723},
	year={2020}
}

@article{Gupta2020,
	title={Overdiagnosis and overtreatment; how to deal with too much medicine},
	author={Gupta, Puneeta and Gupta, Meeta and Koul, Neeraj},
	journal={Journal of Family Medicine and Primary Care},
	volume={9},
	number={8},
	pages={3815--3819},
	year={2020},
	publisher={Medknow}
}

@article{Patz2014,
	title={Overdiagnosis in low-dose computed tomography screening for lung cancer},
	author={Patz, Edward F and Pinsky, Paul and Gatsonis, Constantine and Sicks, JoRean D and Kramer, Barnett S and Tammem{\"a}gi, Martin C and Chiles, Caroline and Black, William C and Aberle, Denise R and NLST Overdiagnosis Manuscript Writing Team and others},
	journal={JAMA internal medicine},
	volume={174},
	number={2},
	pages={269--274},
	year={2014},
	publisher={American Medical Association}
}

@article{Jalalifar2022,
	title={Impact of tumour segmentation accuracy on efficacy of quantitative MRI biomarkers of radiotherapy outcome in brain metastasis},
	author={Jalalifar, Seyed Ali and Soliman, Hany and Sahgal, Arjun and Sadeghi-Naini, Ali},
	journal={Cancers},
	volume={14},
	number={20},
	pages={5133},
	year={2022},
	publisher={MDPI}
}

@article{Jungo2020,
	title={Analyzing the quality and challenges of uncertainty estimations for brain tumor segmentation},
	author={Jungo, Alain and Balsiger, Fabian and Reyes, Mauricio},
	journal={Frontiers in neuroscience},
	volume={14},
	pages={282},
	year={2020},
	publisher={Frontiers Media SA}
}

@article{Topol2019,
	title={High-performance medicine: the convergence of human and artificial intelligence},
	author={Topol, Eric J},
	journal={Nature medicine},
	volume={25},
	number={1},
	pages={44--56},
	year={2019},
	publisher={Nature Publishing Group US New York}
}

@inproceedings{Papadopoulos2002,
	title={Inductive confidence machines for regression},
	author={Papadopoulos, Harris and Proedrou, Kostas and Vovk, Volodya and Gammerman, Alex},
	booktitle={Machine learning: ECML 2002: 13th European conference on machine learning Helsinki, Finland, August 19--23, 2002 proceedings 13},
	pages={345--356},
	year={2002},
	organization={Springer}
}

@article{Shafer2008,
	title={A tutorial on conformal prediction.},
	author={Shafer, Glenn and Vovk, Vladimir},
	journal={Journal of Machine Learning Research},
	volume={9},
	number={3},
	year={2008}
}

@inproceedings{Mossina2024,
	title={Conformal Semantic Image Segmentation: Post-hoc Quantification of Predictive Uncertainty},
	author={Mossina, Luca and Dalmau, Joseba and And{\'e}ol, L{\'e}o},
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages={3574--3584},
	year={2024}
}

@article{Blanchard2024,
	title={FDR control and FDP bounds for conformal link prediction},
	author={Blanchard, Gilles and Durand, Guillermo and Marandon-Carlhian, Ariane and P{\'e}rier, Romain},
	journal={arXiv preprint arXiv:2404.02542},
	year={2024}
}

@book{Vovk2005,
	title={Algorithmic learning in a random world},
	author={Vovk, Vladimir and Gammerman, Alexander and Shafer, Glenn},
	volume={29},
	year={2005},
	publisher={Springer}
}

@inproceedings{Sun2024,
	title={Copula conformal prediction for multi-step time series forecasting},
	author={Sun, Sophia and Yu, Rose},
	booktitle={International Conference on Learning Representations (ICLR)},
	year={2024}
}

@inproceedings{Maple2003,
	title={Geometric design and space planning using the marching squares and marching cube algorithms},
	author={Maple, Carsten},
	booktitle={2003 international conference on geometric modeling and graphics, 2003. Proceedings},
	pages={90--95},
	year={2003},
	organization={IEEE}
}

@inproceedings{De2022,
	title={Object Detection With Probabilistic Guarantees},
	author={de Grancey, Florence and Adam, Jean-Luc and Alecu, Lucian and Gerchinovitz, S{\'e}bastien and Mamalet, Franck and Vigouroux, David},
	booktitle={Fifth International Workshop on Artificial Intelligence Safety Engineering (WAISE 2022)},
	year={2022}
}

@inproceedings{Andeol2023,
	title={Confident Object Detection via Conformal Prediction and Conformal Risk Control: an Application to Railway Signaling},
	author={And{\'e}ol, L{\'e}o and Fel, Thomas and De Grancey, Florence and Mossina, Luca},
	booktitle={Conformal and Probabilistic Prediction with Applications},
	pages={36--55},
	year={2023},
	organization={PMLR}
}

@article{Mukama2024,
	title={Copula-based conformal prediction for object detection: a more efficient approach},
	author={Mukama, Bruce Cyusa and Messoudi, Soundouss and Rousseau, Sylvain and Destercke, S{\'e}bastien},
	journal={Proceedings of Machine Learning Research},
	volume={230},
	pages={1--18},
	year={2024}
}

@article{Tibshirani2019,
	title={Conformal prediction under covariate shift},
	author={Tibshirani, Ryan J and Foygel Barber, Rina and Candes, Emmanuel and Ramdas, Aaditya},
	journal={Advances in neural information processing systems},
	volume={32},
	year={2019}
}

@article{Angelopoulos2021LTT,
	title={Learn then test: Calibrating predictive algorithms to achieve risk control},
	author={Angelopoulos, Anastasios N and Bates, Stephen and Cand{\`e}s, Emmanuel J and Jordan, Michael I and Lei, Lihua},
	journal={arXiv preprint arXiv:2110.01052},
	year={2021}
}

@article{Angelopoulos2022,
	title={Conformal risk control},
	author={Angelopoulos, Anastasios N and Bates, Stephen and Fisch, Adam and Lei, Lihua and Schuster, Tal},
	journal={arXiv preprint arXiv:2208.02814},
	year={2022}
}

@book{Vovk2005,
	title={Algorithmic learning in a random world},
	author={Vovk, Vladimir and Gammerman, Alexander and Shafer, Glenn},
	volume={29},
	year={2005},
	publisher={Springer}
}


@article{Bates2021,
	title={Distribution-free, risk-controlling prediction sets},
	author={Bates, Stephen and Angelopoulos, Anastasios and Lei, Lihua and Malik, Jitendra and Jordan, Michael},
	journal={Journal of the ACM (JACM)},
	volume={68},
	number={6},
	pages={1--34},
	year={2021},
	publisher={ACM New York, NY}
}

@inproceedings{KVASIR2017,
	title = {KVASIR: A Multi-Class Image Dataset for Computer Aided Gastrointestinal Disease Detection},
	author = {
	Pogorelov, Konstantin and Randel, Kristin Ranheim and Griwodz, Carsten and
	Eskeland, Sigrun Losada and de Lange, Thomas and Johansen, Dag and
	Spampinato, Concetto and Dang-Nguyen, Duc-Tien and Lux, Mathias and
	Schmidt, Peter Thelin and Riegler, Michael and Halvorsen, P{\aa}l
	},
	booktitle = {Proceedings of the 8th ACM on Multimedia Systems Conference},
	series = {MMSys'17},
	year = {2017},
	isbn = {978-1-4503-5002-0},
	location = {Taipei, Taiwan},
	pages = {164--169},
	numpages = {6},
	doi = {10.1145/3083187.3083212},
	acmid = {3083212},
	publisher = {ACM},
	address = {New York, NY, USA},
}

@article{Hyperkvasir2020,
	title={HyperKvasir, a comprehensive multi-class image and video dataset for gastrointestinal endoscopy},
	author={Borgli, Hanna and Thambawita, Vajira and Smedsrud, Pia H and Hicks, Steven and Jha, Debesh and Eskeland, Sigrun L and Randel, Kristin Ranheim and Pogorelov, Konstantin and Lux, Mathias and Nguyen, Duc Tien Dang and others},
	journal={Scientific data},
	volume={7},
	number={1},
	pages={283},
	year={2020},
	publisher={Nature Publishing Group UK London}
}

@article{Bernal2012,
	title={Towards automatic polyp detection with a polyp appearance model},
	author={Bernal, Jorge and S{\'a}nchez, Javier and Vilarino, Fernando},
	journal={Pattern Recognition},
	volume={45},
	number={9},
	pages={3166--3182},
	year={2012},
	publisher={Elsevier}
}

@article{Silva2014,
	title={Toward embedded detection of polyps in wce images for early diagnosis of colorectal cancer},
	author={Silva, Juan and Histace, Aymeric and Romain, Olivier and Dray, Xavier and Granado, Bertrand},
	journal={International journal of computer assisted radiology and surgery},
	volume={9},
	pages={283--293},
	year={2014},
	publisher={Springer}
}

@inproceedings{PraNet2020,
	title={Pranet: Parallel reverse attention network for polyp segmentation},
	author={Fan, Deng-Ping and Ji, Ge-Peng and Zhou, Tao and Chen, Geng and Fu, Huazhu and Shen, Jianbing and Shao, Ling},
	booktitle={International conference on medical image computing and computer-assisted intervention},
	pages={263--273},
	year={2020},
	organization={Springer}
}

@article{Filippi2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.00829v2},
author = {Filippi, Sarah and Holmes, Chris C.},
doi = {10.1214/16-BA1027},
eprint = {arXiv:1506.00829v2},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Filippi, Holmes - 2015 - A Bayesian nonparametric approach to testing for dependence between random variables(2).pdf:pdf},
issn = {1936-0975},
keywords = {bayesian nonparametrics,dependence measure,hypothesis,olya tree,p},
title = {{A Bayesian nonparametric approach to testing for dependence between random variables}},
year = {2015}
}
@book{Steinwart2008,
abstract = {This book explains the principles that make support vector machines (SVMs) a successful modelling and prediction tool for a variety of applications. The authors present the basic ideas of SVMs together with the latest developments and current research questions in a unified style. They identify three reasons for the success of SVMs: their ability to learn well with only a very small number of free parameters, their robustness against several types of model violations and outliers, and their computational efficiency compared to several other methods. Since their appearance in the early nineties, support vector machines and related kernel-based methods have been successfully applied in diverse fields of application such as bioinformatics, fraud detection, construction of insurance tariffs, direct marketing, and data and text mining. As a consequence, SVMs now play an important role in statistical machine learning and are used not only by statisticians, mathematicians, and computer scientists, but also by engineers and data analysts. The book provides a unique in-depth treatment of both fundamental and recent material on SVMs that so far has been scattered in the literature. The book can thus serve as both a basis for graduate courses and an introduction for statisticians, mathematicians, and computer scientists. It further provides a valuable reference for researchers working in the field. The book covers all important topics concerning support vector machines such as: loss functions and their role in the learning process; reproducing kernel Hilbert spaces and their properties; a thorough statistical analysis that uses both traditional uniform bounds and more advancedlocalized techniques based on Rademacher averages and Talagrand's inequality; a detailed treatment of classification and regression; a detailed robustness analysis; and a description of some of the most recent implementation techniques. To make the book self-contained, an extensive appendix is added which provides the reader with the necessary background from statistics, probability theory, functional analysis, convex analysis, and topology.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Steinwart, Ingo and Christmann, Andreas},
doi = {10.1007/978-0-387-77242-4},
eprint = {arXiv:1011.1669v3},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Steinwart, Christmann - 2008 - Support Vector Machines(2).pdf:pdf},
isbn = {978-0-387-77241-7},
issn = {1613-9011},
pages = {618},
pmid = {21889629},
title = {{Support Vector Machines}},
year = {2008}
}
@article{Bauer2007,
author = {Bauer, Frank and Pereverzev, Sergei and Rosasco, Lorenzo},
doi = {10.1016/j.jco.2006.07.001},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bauer, Pereverzev, Rosasco - 2007 - On regularization algorithms in learning theory(2).pdf:pdf},
keywords = {- see front matter,0885-064x,16146 genova,2006 elsevier inc,all rights reserved,corresponding author,disi,dodecaneso 35,e-mail address,it,italy,l,learning theory,non-parametric statistics,regularization theory,rosasco,unige,universit{\`{a}} di genova,v},
pages = {52--72},
title = {{On regularization algorithms in learning theory}},
volume = {23},
year = {2007}
}
@book{Anderson1958,
author = {Anderson, Theodore W.},
doi = {10.2307/2343317},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Anderson - 1958 - An Introduction to Multivariate Statistical Analysis(2).pdf:pdf},
isbn = {3175723993},
issn = {0471360910 9780471360919},
pages = {675},
pmid = {21602047},
title = {{An Introduction to Multivariate Statistical Analysis}},
year = {1958}
}
@article{James2002,
abstract = {To cite this article: Gareth M. (2002) with functional predictors Journal of the Royal Statistical Society: Series B},
author = {James, Gareth M.},
doi = {10.1111/1467-9868.00342},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/James - 2002 - Generalized linear models with functional predictors(2).pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Censored regression,Functional data analysis,Functional principal components,Generalized linear models,Logistic regression},
number = {3},
pages = {411--432},
title = {{Generalized linear models with functional predictors}},
volume = {64},
year = {2002}
}
@article{Battiti1994,
abstract = {This paper investigates the application of the mutual information criterion to evaluate a set of candidate features and to select an informative subset to be used as input data for a neural network classifier. Because the mutual information measures arbitrary dependencies between random variables, it is suitable for assessing the `'information content'' of features in complex classification tasks, where methods bases on linear relations (like the correlation) are prone to mistakes. The fact that the mutual information is independent of the coordinates chosen permits a robust estimation. Nonetheless, the use of the mutual information for tasks characterized by high input dimensionality requires suitable approximations because of the prohibitive demands on computation and samples. An algorithm is proposed that is based on a `'greedy'' selection of the features and that takes both the mutual information with respect to the output class and with respect to the already-selected features into account. Finally the results of a series of experiments are discussed.},
author = {Battiti, R},
doi = {10.1109/72.298224},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Battiti - 1994 - Using Mutual Information for Selecting Features in Supervised Neural-Net Learning(2).pdf:pdf},
isbn = {1045-9227},
issn = {1045-9227},
journal = {Ieee Transactions on Neural Networks},
number = {4},
pages = {537--550},
pmid = {18267827},
title = {{Using Mutual Information for Selecting Features in Supervised Neural-Net Learning}},
volume = {5},
year = {1994}
}
@article{Samarov1993,
author = {Samarov, Alexander M.},
doi = {10.2307/2290772},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Samarov - 1993 - Exploring Regression Structure Using Nonparametric Functional Estimation(2).pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {kernel},
number = {423},
pages = {836},
title = {{Exploring Regression Structure Using Nonparametric Functional Estimation}},
volume = {88},
year = {1993}
}
@article{Smale2005,
author = {Smale, Steve},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Smale - 2005 - Learning Theory Estimates via Integral Operators and Their Approximations.pdf:pdf},
pages = {1--20},
title = {{Learning Theory Estimates via Integral Operators and Their Approximations}},
year = {2005}
}
@book{Cover2005,
abstract = {Following a brief introduction and overview, early chapters cover the basic algebraic relationships of entropy, relative entropy and mutual information, AEP, entropy rates of stochastics processes and data compression, duality of data compression and the growth rate of wealth. Later chapters explore Kolmogorov complexity, channel capacity, differential entropy, the capacity of the fundamental Gaussian channel, the relationship between information theory and statistics, rate distortion and network information theories. The final two chapters examine the stock market and inequalities in information theory. In many cases the authors actually describe the properties of the solutions before the presented problems.},
archivePrefix = {arXiv},
arxivId = {ISBN 0-471-06259-6},
author = {Cover, Thomas M. and Thomas, Joy A.},
booktitle = {Elements of Information Theory},
doi = {10.1002/047174882X},
eprint = {ISBN 0-471-06259-6},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cover, Thomas - 2005 - Elements of Information Theory(2).pdf:pdf},
isbn = {9780471241959},
issn = {15579654},
pages = {1--748},
pmid = {20660925},
title = {{Elements of Information Theory}},
year = {2005}
}
@article{Brill1992,
abstract = {ABSTRACT - The task of classifiers is to determine the appropriate class name when presented with a sample from one of several classes. In forming the sample to present to the classifier, there may be a large number of measurements one can make. Feature selection addresses the problem of determining which of these measurements are the most useful for determining the pattern's class. In this paper, we describe experiments using a genetic algorithm for feature selection in the context of neural network classifiers, specifically, counterpropagation networks. We present two novel techniques in our application of genetic algorithms. First, we configure our genetic algorithm to use an approximate evaluation in order to reduce significantly the computation required. In particular, though our desired classifiers are counterpropagation networks, we use a nearest-neighbor classifier to evaluate feature sets. We show that the features selected by this method are effective in the context of counterpropagation networks. Second, we propose a method we call training set sampling, in which only a portion of the training set is used on any given evaluation. Again, significant computational savings can be made by using this method, i.e., evaluations can be made over an order of magnitude faster. This method selects feature sets that are as good as and occasionally better for counterpropagation than those chosen by an evaluation that uses the entire training set.},
author = {Brill, Frank Z. and Brown, Donald E. and Martin, Worthy N.},
doi = {10.1109/72.125874},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brill, Brown, Martin - 1992 - Fast Genetic Selection of Features for Neural Network Classifiers(2).pdf:pdf},
isbn = {1045-9227},
issn = {19410093},
journal = {IEEE Transactions on Neural Networks},
number = {2},
pages = {324--328},
pmid = {18276434},
title = {{Fast Genetic Selection of Features for Neural Network Classifiers}},
volume = {3},
year = {1992}
}
@article{Holmes2015,
abstract = {In this article we describe Bayesian nonparametric procedures for two-sample hypothesis testing. Namely, given two sets of samples y{\^{}}{\{}(1){\}} iid F{\^{}}{\{}(1){\}} and y{\^{}}{\{}(2){\}} iid F{\^{}}{\{}(2){\}}, with F{\^{}}{\{}(1){\}}, F{\^{}}{\{}(2){\}} unknown, we wish to evaluate the evidence for the null hypothesis H{\_}{\{}0{\}}:F{\^{}}{\{}(1){\}} = F{\^{}}{\{}(2){\}} versus the alternative. Our method is based upon a nonparametric Polya tree prior centered either subjectively or using an empirical procedure. We show that the Polya tree prior leads to an analytic expression for the marginal likelihood under the two hypotheses and hence an explicit measure of the probability of the null Pr(H{\_}{\{}0{\}}|y{\^{}}{\{}(1){\}},y{\^{}}{\{}(2){\}}).},
archivePrefix = {arXiv},
arxivId = {0910.5060},
author = {Holmes, Chris C. and Caron, Fran{\c{c}}ois and Griffin, Jim E. and Stephens, David A.},
doi = {10.1214/14-BA914},
eprint = {0910.5060},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Holmes et al. - 2015 - Two-sample Bayesian nonparametric hypothesis testing(2).pdf:pdf},
issn = {19316690},
journal = {Bayesian Analysis},
keywords = {Bayesian nonparametrics,Hypothesis testing,P??lya tree},
number = {2},
pages = {297--320},
title = {{Two-sample Bayesian nonparametric hypothesis testing}},
volume = {10},
year = {2015}
}
@article{Faraway1997,
author = {Faraway, J.J.},
doi = {10.2307/1271130},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Faraway - 1997 - Regression analysis for a functional response(2).pdf:pdf},
issn = {00401706},
journal = {Technometrics},
keywords = {curve estimation,designed for the situ-,ergonomics,functional data analysis,functional regression analysis is,longitudinal data anal-,nonparametric regression,repeated measures,ysis},
number = {3},
pages = {254--261},
pmid = {220},
title = {{Regression analysis for a functional response}},
volume = {39},
year = {1997}
}
@misc{Golub1996,
abstract = {Revised and updated, the third edition of Golub and Van Loan's classic text in computer science provides essential information about the mathematical background and algorithmic skills required for the production of numerical software. This new edition includes thoroughly revised chapters on matrix multiplication problems and parallel matrix computations, expanded treatment of CS decomposition, an updated overview of floating point arithmetic, a more accurate rendition of the modified Gram-Schmidt process, and new material devoted to GMRES, QMR, and other methods designed to handle the sparse unsymmetric linear system problem.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Golub, G H and {Van Loan}, C F},
booktitle = {Physics Today},
doi = {10.1063/1.3060478},
eprint = {arXiv:1011.1669v3},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Golub, Van Loan - 1996 - Matrix Computations(2).pdf:pdf},
isbn = {0801854148},
issn = {00036935},
number = {8},
pages = {48},
pmid = {18273219},
title = {{Matrix Computations}},
volume = {10},
year = {1996}
}
@article{Gareth2009,
abstract = {Regression models to relate a scalar {\$}Y{\$} to a functional predictor {\$}X(t){\$} are becoming increasingly common. Work in this area has concentrated on estimating a coefficient function, {\$}\backslashbeta(t){\$}, with {\$}Y{\$} related to {\$}X(t){\$} through {\$}\backslashint\backslashbeta(t)X(t) dt{\$}. Regions where {\$}\backslashbeta(t)\backslashne0{\$} correspond to places where there is a relationship between {\$}X(t){\$} and {\$}Y{\$}. Alternatively, points where {\$}\backslashbeta(t)=0{\$} indicate no relationship. Hence, for interpretation purposes, it is desirable for a regression procedure to be capable of producing estimates of {\$}\backslashbeta(t){\$} that are exactly zero over regions with no apparent relationship and have simple structures over the remaining regions. Unfortunately, most fitting procedures result in an estimate for {\$}\backslashbeta(t){\$} that is rarely exactly zero and has unnatural wiggles making the curve hard to interpret. In this article we introduce a new approach which uses variable selection ideas, applied to various derivatives of {\$}\backslashbeta(t){\$}, to produce estimates that are both interpretable, flexible and accurate. We call our method "Functional Linear Regression That's Interpretable" (FLiRTI) and demonstrate it on simulated and real-world data sets. In addition, non-asymptotic theoretical bounds on the estimation error are presented. The bounds provide strong theoretical motivation for our approach.},
archivePrefix = {arXiv},
arxivId = {0908.2918},
author = {James, Gareth M. and Wang, Jing and Zhu, Ji},
doi = {10.1214/08-AOS641},
eprint = {0908.2918},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/James, Wang, Zhu - 2009 - Functional linear regression that's interpretable(2).pdf:pdf},
isbn = {0090-5364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Dantzig selector,Functional linear regression,Interpretable regression,Lasso},
number = {5 A},
pages = {2083--2108},
title = {{Functional linear regression that's interpretable}},
volume = {37},
year = {2009}
}
@article{Szekely2007,
abstract = {Distance correlation is a new measure of dependence between random vectors. Distance covariance and distance correlation are analogous to product-moment covariance and correlation, but unlike the classical definition of correlation, distance correlation is zero only if the random vectors are independent. The empirical distance dependence measures are based on certain Euclidean distances between sample elements rather than sample moments, yet have a compact representation analogous to the classical covariance and correlation. Asymptotic properties and applications in testing independence are discussed. Implementation of the test and Monte Carlo results are also presented.},
archivePrefix = {arXiv},
arxivId = {0803.4101},
author = {Sz{\'{e}}kely, G{\'{a}}bor J. and Rizzo, Maria L. and Bakirov, Nail K.},
doi = {10.1214/009053607000000505},
eprint = {0803.4101},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sz{\'{e}}kely, Rizzo, Bakirov - 2007 - Measuring and Testing Dependence by Correlation of Distances(2).pdf:pdf},
isbn = {0090-5364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Distance correlation,Distance covariance,Kernels,Multivariate independence},
mendeley-tags = {Kernels},
number = {6},
pages = {2769--2794},
title = {{Measuring and Testing Dependence by Correlation of Distances}},
volume = {35},
year = {2007}
}
@article{James1993,
author = {James, Gareth M. and Silverman, Bernard W.},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/James, Silverman - 1993 - Functional Adaptive Model Estimation(2).pdf:pdf},
keywords = {Functional predictor,Functional principal components,Generalized,Generalized linear models,Projection pursuit regression,additive models},
pages = {1--20},
title = {{Functional Adaptive Model Estimation}},
year = {1993}
}
