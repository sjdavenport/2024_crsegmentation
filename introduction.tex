Deep neural networks promise to significantly enhance a wide range of important tasks in biomedical imaging. However these models, as typically used, lack formal uncertainty guarantees on their output which can lead to overconfident predictions and critical errors \citep{Guo2017, Gupta2020}. Misclassifications or inaccurate segmentations can lead to serious consequences, including misdiagnosis, inappropriate treatment decisions, or missed opportunities for early intervention \citep{Topol2019}. Without uncertainty quantification, medical professionals cannot rely on deep learning models to provide accurate information and predictions which can limit their use in practical applications \citep{Jungo2020}. 
%As these models are increasingly deployed in critical real-world scenarios quantifying the uncertainty associated with their predictions is a significant challenge.

In order to address this problem, conformal inference, a robust framework for uncertainty quantification, has become increasingly used as a means of providing prediction guarantees, offering reliable, distribution-free confidence sets for the output of neural networks which have finite sample validity. This approach, originally introduced in \cite{Papadopoulos2002, Vovk2005}, has become increasingly popular due to its ability to provide rigorous statistical guarantees without making strong assumptions about the underlying data distribution or model architecture. Conformal prediction methods, in their most commonly used form - split conformal inference - work by calibrating the predictions of the model on a held-out dataset in order to provide sets which contain the output with a given probability, see \cite{Shafer2008} and \cite{Angelopoulos2021} for good introductions.

In the context of image segmentation, we have a decision to make at each pixel/voxel of an image which can lead to a large multiple testing problem. Traditional conformal methods, typically designed for scalar outputs, require adaptation to handle multiple tests and their inherent spatial dependencies. To do so \cite{Angelopoulos2021LTT} applied conformal inference pixelwise and performed multiple testing correction on the resulting $p$-values, however this approach does not account for the complex dependence structure inherent in the images. To take advantage of this structure, in an approach analogous to the \nt{False discovery rate (FDR)} control of \citep{Benjamini1995}, \cite{Bates2021} and \cite{Angelopoulos2022} sought to control the expected risk of a given loss function over the image and used a conformal approach to produce outer confidence sets for segmented images which control the expected proportion of false negatives. Other work considering conformal inference in the context of multiple dependent hypotheses includes \cite{Marandon2024} and \cite{Blanchard2024} who established conformal FDR control when testing for the presence of missing links in graphs.

In this work we argue that bounding the segmented outcome with guarantees in probability rather than on the proportion of discoveries is more informative, avoiding errors at the borders of potential growths/tumors. This is analogous to the tradeoff between \nt{familywise error rate (FWER) and FDR} control in the multiple testing literature in which there is a balance between power and coverage rate, \nt{(a correspondence which we formalize in Section \ref{coveragerates})}. The distinction is that in medical image segmentation making mistakes can have potentially serious consequences Under-segmentation might cause part of the true mask to be missed, potentially leading to inadequate treatment \citep{Jalalifar2022}. Over-segmentation, on the other hand, could result in unnecessary interventions, increasing patient risk and healthcare costs \citep{Gupta2020, Patz2014}. Confidence sets are instead guaranteed to contain the outcome with a given level of certainty. Since the guarantees are more meaningful the problem is more difficult and existing work on conformal uncertainty quantification for images has thus often focused on producing sets with guarantees on the proportions of discoveries or pixel level inference rather than coverage (\cite{Bates2021}, \cite{Wieslander2020}, \cite{Mossina2024}) which is a stricter error criterion. 
%Existing work on conformal confidence sets which aim to provide coverage of the entire ground truth mask with a given probability has primarily focused on bounding boxes, see e.g. \citep{De2022, Andeol2023, Mukama2024}. 
%In order to address this, we use a held out learning dataset to learn the score transformations which provide the most informative confidence regions.

In order to obtain confidence sets we use a split-conformal inference approach in which we learn appropriate cutoffs, with which to threshold the output of an image segmenter, from a calibration dataset. These thresholds are obtained by considering the distribution of the maximum logit (transformed) scores provided by the model within and outside of the ground truth masks. This approach allows us to capture the spatial nature of the uncertainty in segmentation tasks, going beyond simple pixel-wise confidence measures. By applying these learned thresholds to new predictions, we can generate inner and outer confidence sets that are guaranteed to contain the true, unknown segmented mask with a desired probability. As we shall see, naively using the original logit scores to do so can lead to rather large and uninformative outer confidence sets but these can be greatly improved using distance transformations. 
%In the following sections, we will explore the technical details of our method, present our theoretical results, and illustrate and validate our approach on a polyps tumor dataset. In particular Section XXX provides the theory for constructing joint and marginal conformal confidence sets and includes an extension to full conformal inference. We provide theoretical guarantees on the coverage properties of our confidence sets, ensuring their reliability across different datasets and segmentation models. In Section XXX, we learn appropriate score transformations on a set aside learning dataset. We perform conformal inference on a clibrati
 