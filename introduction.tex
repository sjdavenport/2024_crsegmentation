


Deep neural networks promise to significantly enhance a wide range of important tasks in biomedical imaging. However these models, as typically used, lack formal uncertainty guarantees on their output which can lead to overconfident predictions and critical errors \citep{Guo2017, Gupta2020}. Misclassifications or inaccurate segmentations can lead to serious consequences, including misdiagnosis, inappropriate treatment decisions, or missed opportunities for early intervention \citep{Topol2019}. Without uncertainty quantification, medical professionals cannot rely on deep learning models to provide accurate information and predictions which can limit their use in practical applications \citep{Jungo2020}. 
%As these models are increasingly deployed in critical real-world scenarios quantifying the uncertainty associated with their predictions is a significant challenge.

In order to address this problem, conformal inference, a robust framework for uncertainty quantification, has become increasingly used as a means of providing prediction guarantees, offering reliable, distribution-free confidence sets for the output of neural networks which have finite sample validity. This approach, originally introduced in \cite{Papadopoulos2002, Vovk2005}, has become increasingly popular due to its ability to provide rigorous statistical guarantees without making strong assumptions about the underlying data distribution or model architecture. Conformal prediction methods, in their most commonly used form - split conformal inference - work by calibrating the predictions of the model on a held-out dataset in order to provide sets which contain the output with a given probability, see \cite{Shafer2008} and \cite{Angelopoulos2021} for a good introduction.

In the context of image segmentation, we have a decision to make at each pixel/voxel of an image which can lead to a large multiple testing problem. Traditional conformal methods, typically designed for scalar outputs, require adaptation to handle multiple tests and their inherent spatial dependencies. To do so \cite{Angelopoulos2021LTT} applied conformal inference pixelwise and performed multiple testing correction on the resulting $p$-values, however this approach does not take into account of the complex dependence structure inherent in the images. To take advantage of this structure, in an approach analogous to the FDR control of \citep{Benjamini1995}, \cite{Bates2021} and \cite{Angelopoulos2022} sought to control the expected risk of a given loss function over the image and used a conformal approach to produce outer confidence sets for segmented images which control the expected false negative rate. Other work considering conformal inference in the context of multiple dependent hypotheses include \cite{Marandon2024} and \cite{Blanchard2024} who established conformal FDR control when testing for the presence of missing links in graphs.

In this work we argue that bounding the segmented outcome with guarantees in probability rather than in expectation/proportion can be more informative, avoiding errors at the borders of potential tumors. This is analogous to the tradeoff between FWER and FDR/FDP control in the multiple testing literature in which there is a balance between power and coverage rate, the distinction being that in medical image segmentation there can be a potentially serious consequence to making mistakes. Under-segmentation might cause part of the tumor to be missed, potentially leading to inadequate treatment. Over-segmentation, on the other hand, could result in unnecessary interventions, increasing patient risk and healthcare costs \citep{Gupta2020, Patz2014}. Unlike bounds on the proportion of discovered pixels/voxels, confidence sets are guaranteed to contain the outcome with a given level of confidence and allow medical practitioners to follow-up on the images where there is greater uncertainty. Since the guarantees are more meaningful the problem is more difficult and existing work has thus often focused on producing sets with guarantees on the proportions of discoveries rather than coverage (e.g. \cite{Bates2021}) as coverage is a stricter error criterion \citep{Mossina2024}. Indeed, as we shall see, using the original scores can lead to rather large and uninformative outer confidence sets. In order to address this, we use a held out learning dataset to learn the score transformations which provide the most informative confidence regions.

In order to obtain confidence sets we use a split-conformal inference approach in which we learn appropriate cutoffs, with which to threshold the output of an image segmenter, from a calibration dataset. These thresholds are obtained by considering the distribution of the maximum logit (transformed) scores provided by the model within and outside of the ground truth masks. This approach allows us to capture the spatial nature of the uncertainty in segmentation tasks, going beyond simple pixel-wise confidence measures. By applying these learned thresholds to new predictions, we can generate inner and outer confidence sets that are guaranteed to contain the true, unknown segmented mask with a desired probability. 
%In the following sections, we will explore the technical details of our method, present our theoretical results, and illustrate and validate our approach on a polpys tumor dataset. In particular Section XXX provides the theory for constructing joint and marginal conformal confidence sets and includes an extension to full conformal inference. We provide theoretical guarantees on the coverage properties of our confidence sets, ensuring their reliability across different datasets and segmentation models. In Section XXX, we learn appropriate score transformations on a set aside learning dataset. We perform conformal inference on a clibrati
 