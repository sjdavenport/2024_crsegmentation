In this work, we have developed conformal confidence sets which offer probabilistic guarantees for the output of a black box image segmentation model and provide tight bounds. Our work helps to address the lack of formal uncertainty quantification in the application of deep neural networks to medical imaging which has limited the reliability and adoption of these models in practice. The use of improved neural networks which can better separate the scores within and outside the ground truth masks would lead to more precise confidence sets and optimizing this is an important area of research. We have here established validity guarantees and additionally showed that these can be used to theoretically justify a modified version of the max-additive bounding box based method of \cite{Andeol2023}. 

The use of the distance transformed scores was crucial in providing tight outer confidence bounds as the original neural network is by itself unable to robustly determine where the tumors end with certainty.  The distance transformation penalizes regions away from the predicted mask, allowing tumor regions to be distinguished from the background. In other datasets and model settings, other transformations may be appropriate. As such we strongly recommend the use of a learning dataset in order to calibrate the transformations and maximize precision of the resulting confidence bounds.

The confidence sets we develop in this paper are related in spirit to work on uncertainty quantification for spatial excursion sets (\cite{Bowring2019}, \cite{Mejia2020}, \cite{chen2017density}). These approaches instead assume that multiple observations from a signal plus noise model are observed and perform inference on the underlying signal rather than prediction. They rely on central limit theorems or distributional assumptions in order to provide spatial confidence regions with asymptotic coverage guarantees. 

\section*{Availability of code}
\vspace{-0.1cm}
Matlab code to implement the methods of this paper and a demo on a downscaled version of the data is available in the supplementary material. The code is very fast: calculating inner and outer thresholds (over the 1000 images in the calibration set) requires approximately 0.03 seconds on the downscaled data on a standard laptop (Apple M3 chip with 16 GB RAM) and taking 2.64 seconds for the original dataset.